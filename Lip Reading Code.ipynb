{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-08T18:03:20.411093Z","iopub.status.busy":"2023-12-08T18:03:20.410253Z","iopub.status.idle":"2023-12-08T18:03:20.418746Z","shell.execute_reply":"2023-12-08T18:03:20.417751Z","shell.execute_reply.started":"2023-12-08T18:03:20.411056Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.preprocessing import MinMaxScaler\n","\n","#code pour recup les raw data\n","\n","# LOAD DES DONNEES\n","\n","# Fonction pour charger les données à partir d'un dossier\n","def load_data(folder):\n","    data = []\n","    \n","    # Parcourez tous les fichiers CSV dans le dossier principal\n","    for label, file in enumerate(os.listdir(folder)):\n","        file_path = os.path.join(folder, file)\n","        \n","        # Vérifiez si l'élément est un fichier CSV\n","        if file.endswith(\".csv\") and os.path.isfile(file_path):\n","            df = pd.read_csv(file_path)\n","            \n","            # Aplatir les données et les ajouter à la liste\n","            data.append(df.values.flatten())  \n","    for i in range(len(data)) :    # Séparer les éléments des sous-listes en sous-listes pour que test_data[i][j][k] corresponde au ième fichier, jème ligne et un sélectionne x, y, polarity ou time avec k. \n","        data[i] = np.array(data[i]).reshape(-1, 4).tolist()\n","    return data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:03:26.713329Z","iopub.status.busy":"2023-12-08T18:03:26.712948Z","iopub.status.idle":"2023-12-08T18:06:06.527994Z","shell.execute_reply":"2023-12-08T18:06:06.5269Z","shell.execute_reply.started":"2023-12-08T18:03:26.713298Z"},"trusted":true},"outputs":[],"source":["def create_data_dictionary(folder_path):\n","    dict_data = {}\n","\n","    for i, subfolder in enumerate(os.listdir(folder_path)):\n","        subfolder_path = os.path.join(folder_path, subfolder)\n","        data_list = load_data(subfolder_path)\n","        dict_data[str(i)] = data_list\n","\n","    return dict_data\n","\n","# Example usage\n","folder_path = '/kaggle/input/smemi309-final-evaluation-challenge-2023/train10/train10'\n","data_dict = create_data_dictionary(folder_path)\n","\n","# Accessing values in the data_dict\n","for label, data_list in data_dict.items():\n","    print(f\"Label: {label}, Data List Length: {len(data_list)}\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:06.530046Z","iopub.status.busy":"2023-12-08T18:06:06.529669Z","iopub.status.idle":"2023-12-08T18:06:47.020892Z","shell.execute_reply":"2023-12-08T18:06:47.01977Z","shell.execute_reply.started":"2023-12-08T18:06:06.530016Z"},"trusted":true},"outputs":[],"source":["# Example usage\n","folder_path = '/kaggle/input/smemi309-final-evaluation-challenge-2023/test10'\n","data_dict_test = create_data_dictionary(folder_path)\n","\n","# Accessing values in the data_dict\n","for label, data_list in data_dict_test.items():\n","    print(f\"Label: {label}, Data List Length: {len(data_list)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Preprocessing des données"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.022441Z","iopub.status.busy":"2023-12-08T18:06:47.022139Z","iopub.status.idle":"2023-12-08T18:06:47.029267Z","shell.execute_reply":"2023-12-08T18:06:47.028321Z","shell.execute_reply.started":"2023-12-08T18:06:47.022414Z"},"trusted":true},"outputs":[],"source":["def interv(data,n): # Sépare le dataframe en n intervalle de temps\n","    Temps = []\n","    L = len(data)\n","    for i in range(n):\n","        Temps.append(i * len(data) // n)\n","    Temps.append(L-1)\n","    return Temps\n","\n","def separedata(data,n):\n","    D = []\n","    S = interv(data,n)\n","    for i in range(1,len(S)):\n","        D.append(data[S[i-1]:S[i]])\n","    return D"]},{"cell_type":"markdown","metadata":{},"source":["Mise sous form matricielle des données"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.03205Z","iopub.status.busy":"2023-12-08T18:06:47.031734Z","iopub.status.idle":"2023-12-08T18:06:47.043666Z","shell.execute_reply":"2023-12-08T18:06:47.042723Z","shell.execute_reply.started":"2023-12-08T18:06:47.032023Z"},"trusted":true},"outputs":[],"source":["def matr(data):   #n = nombre d'intervalle    (à faire avec data = test_data[0])\n","    list_data = []\n","    for j in range(len(data)):\n","        list_data.append([data[j][0], data[j][1], data[j][2]])      #[[1,2,3],[4,5,6],[7,8,9]]\n","    M = np.zeros([224,90])\n","    for i in range(len(data)):\n","            if list_data[i][2] == 1 :\n","                M[list_data[i][0],list_data[i][1]] += 1   #M[x,y] = somme des polarités\n","            else :\n","                M[list_data[i][0],list_data[i][1]] -= 1\n","    return M"]},{"cell_type":"markdown","metadata":{},"source":["Convolution des matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.045189Z","iopub.status.busy":"2023-12-08T18:06:47.044746Z","iopub.status.idle":"2023-12-08T18:06:47.054843Z","shell.execute_reply":"2023-12-08T18:06:47.053902Z","shell.execute_reply.started":"2023-12-08T18:06:47.045159Z"},"trusted":true},"outputs":[],"source":["def sum_by_block(M,Lx,Ly):\n","\n","    # Calculer les dimensions de la nouvelle matrice B\n","    nb_lignes_B = M.shape[0] // Lx\n","    nb_colonnes_B = M.shape[1] // Ly\n","\n","    # Initialiser la nouvelle matrice B avec des zéros\n","    B = np.zeros((nb_lignes_B, nb_colonnes_B), dtype=M.dtype)\n","\n","    # Parcourir les blocs de la matrice M\n","    for i in range(nb_lignes_B):\n","        for j in range(nb_colonnes_B):\n","            # Extraire le bloc correspondant de la matrice M avec les longueurs Lx et Ly\n","            bloc_M = M[i * Lx:(i + 1) * Lx, j * Ly:(j + 1) * Ly]\n","\n","            # Ajouter la somme des éléments du bloc à la matrice B\n","            B[i, j] = np.sum(bloc_M)\n","\n","    # Sors la matrice résultante B\n","    return B\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.057688Z","iopub.status.busy":"2023-12-08T18:06:47.055996Z","iopub.status.idle":"2023-12-08T18:06:47.065086Z","shell.execute_reply":"2023-12-08T18:06:47.06423Z","shell.execute_reply.started":"2023-12-08T18:06:47.057661Z"},"trusted":true},"outputs":[],"source":["def label_bloc(data, n, Lx, Ly) :\n","    M = []\n","    for i in range(32) :\n","        M.append([])\n","        for j in range(n) :\n","            M[i].append(sum_by_block(matr(separedata(data[i],n)[j]), Lx, Ly))\n","    return M"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.066944Z","iopub.status.busy":"2023-12-08T18:06:47.066292Z","iopub.status.idle":"2023-12-08T18:06:47.075376Z","shell.execute_reply":"2023-12-08T18:06:47.074503Z","shell.execute_reply.started":"2023-12-08T18:06:47.066918Z"},"trusted":true},"outputs":[],"source":["def label_bloc_test(data, n, Lx, Ly) :\n","    M = []\n","    for i in range(100) :\n","        M.append([])\n","        for j in range(n) :\n","            M[i].append(sum_by_block(matr(separedata(data[i],n)[j]), Lx, Ly))\n","    return M"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:06:47.076882Z","iopub.status.busy":"2023-12-08T18:06:47.076573Z","iopub.status.idle":"2023-12-08T18:10:43.879042Z","shell.execute_reply":"2023-12-08T18:10:43.878053Z","shell.execute_reply.started":"2023-12-08T18:06:47.076836Z"},"trusted":true},"outputs":[],"source":["n = 20 # Nombre d'intervalles\n","Lx = 28 # Longueur bloc\n","Ly = 15 # Largeur bloc\n","\n","dict_label_bloc = {}\n","\n","for key in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n","    dict_label_bloc[key] = label_bloc(data_dict[key], n, Lx, Ly)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:10:43.880611Z","iopub.status.busy":"2023-12-08T18:10:43.880284Z","iopub.status.idle":"2023-12-08T18:11:51.306469Z","shell.execute_reply":"2023-12-08T18:11:51.305593Z","shell.execute_reply.started":"2023-12-08T18:10:43.880582Z"},"trusted":true},"outputs":[],"source":["n = 20 # Nombre d'intervalles\n","Lx = 28 # Longueur bloc\n","Ly = 15 # Largeur bloc\n","\n","dict_label_bloc_test = {}\n","dict_label_bloc_test['0'] = label_bloc_test(data_dict_test['0'], n, Lx, Ly)\n"]},{"cell_type":"markdown","metadata":{},"source":["POUR LE TRAIN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:11:51.310486Z","iopub.status.busy":"2023-12-08T18:11:51.30974Z","iopub.status.idle":"2023-12-08T18:11:51.327128Z","shell.execute_reply":"2023-12-08T18:11:51.326243Z","shell.execute_reply.started":"2023-12-08T18:11:51.310431Z"},"trusted":true},"outputs":[],"source":["data_flat = [element for sous_liste in dict_label_bloc.values() for element in sous_liste]\n","print(np.shape(data_flat))\n","data_flat=np.array(data_flat)\n","data_flat2=data_flat.reshape(320,-1)\n","print(np.shape(data_flat2))"]},{"cell_type":"markdown","metadata":{},"source":["POUR LE TEST\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:11:51.328559Z","iopub.status.busy":"2023-12-08T18:11:51.328191Z","iopub.status.idle":"2023-12-08T18:11:51.339158Z","shell.execute_reply":"2023-12-08T18:11:51.338124Z","shell.execute_reply.started":"2023-12-08T18:11:51.328528Z"},"trusted":true},"outputs":[],"source":["data_flat_test = [element for sous_liste in dict_label_bloc_test.values() for element in sous_liste]\n","print(np.shape(data_flat_test))\n","data_flat_test=np.array(data_flat_test)\n","data_flat2_test=data_flat_test.reshape(100,-1)\n","print(np.shape(data_flat2_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#RANDOM FOREST CLASSFIER V1\n","#Tester pour diffrentes valeurs de n, Lx,Ly et n_estimator\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Chargez les données du dictionnaire\n","def randomforest(dictionnaire,n_est):\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat=np.array(data_flat)\n","    data_flat2=data_flat.reshape(320,-1)\n","\n","    lab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i]=np.array(lab[i])\n","    # Séparez les données en ensemble de test et d'apprentissage\n","\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.4, random_state=7)\n","\n","    # Créez un modèle Random Forest fair varier le n__estimator\n","    clf = RandomForestClassifier(n_estimators=n_est)\n","\n","    # Entraînez le modèle sur l'ensemble d'apprentissage\n","    clf.fit(X_train, y_train)\n","\n","    # Prédisez les sorties pour l'ensemble de test\n","    y_pred = clf.predict(X_test)\n","\n","    # Calculez la précision du modèle\n","    print(\"Précision :\", accuracy_score(y_test, y_pred))\n","    return accuracy_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#RANDOM FOREST CLASSFIER\n","#Tester pour diffrentes valeurs de n, Lx,Ly et n_estimator\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def RandomForest(dictionnaire, Nb_estim, Random):\n","    # Chargez les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat=np.array(data_flat)\n","    data_flat2=data_flat.reshape(320,-1)\n","\n","    lab=np.array([[0]*32,[1]*32,[2]*32,[3]*32,[4]*32,[5]*32,[6]*32,[7]*32,[8]*32,[9]*32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i]=np.array(lab[i])\n","    # Séparez les données en ensemble de test et d'apprentissage\n","\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=Random)\n","\n","    # Créez un modèle Random Forest fair varier le n__estimator\n","    clf = RandomForestClassifier(n_estimators=Nb_estim)\n","\n","    # Entraînez le modèle sur l'ensemble d'apprentissage\n","    clf.fit(X_train, y_train)\n","\n","    # Prédisez les sorties pour l'ensemble de test\n","    y_pred = clf.predict(X_test)\n","\n","    # Calculez la précision du modèle\n","    \"\"\"print(\"Précision :\", accuracy_score(y_test, y_pred))\"\"\"\n","    return accuracy_score(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["Gridsearch de parametre optimal"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","def RandomForestGridSearch(dictionnaire, Random_state):\n","    # Chargez les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparez les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=random_state)\n","\n","    # Paramètres à tester\n","    param_grid = {'n_estimators': np.arange(600,1000,50)}\n","\n","    # Créez un modèle Random Forest\n","    clf = RandomForestClassifier()\n","\n","    # Créez un objet GridSearchCV\n","    grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n","\n","    # Entraînez le modèle sur l'ensemble d'apprentissage\n","    grid_search.fit(X_train, y_train)\n","\n","    # Meilleurs paramètres\n","    best_params = grid_search.best_params_\n","\n","    # Utilisez le meilleur modèle pour prédire les sorties pour l'ensemble de test\n","    y_pred = grid_search.predict(X_test)\n","\n","    # Calculez la précision du modèle avec les meilleurs paramètres\n","    best_accuracy = accuracy_score(y_test, y_pred)\n","\n","    return best_params, best_accuracy\n","\n","# Exemple d'utilisation\n","random_state = 42  # Vous pouvez choisir n'importe quelle valeur pour la reproductibilité\n","best_params, best_accuracy = RandomForestGridSearch(dict_label_bloc, 7)\n","\n","print(\"Meilleurs paramètres:\", best_params)\n","print(\"Précision avec meilleurs paramètres:\", best_accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["Graphe"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Graphe RandomForest en fonction du nombre d'estimateurs\n","\n","import matplotlib.pyplot as plt\n","\n","N = np.arange(25,1200,25)\n","RF = []\n","for i in range(len(N)):\n","    moy = []\n","    for j in range(10):\n","        moy.append(RandomForest(dict_label_bloc, N[i], i)) # random_state = j\n","    moy = np.mean(moy)\n","    RF.append(moy)\n","\n","plt.plot(N, RF, color = 'green', marker = 'o', label = 'Random Forest')\n","plt.legend()\n","\n","print('Précision maximum :', np.max(RF), ' atteint pour', N[np.argmax(RF)],' estimateurs')\n","print('Variance de la précision RandomForest :', np.var(RF))"]},{"cell_type":"markdown","metadata":{},"source":["random forest avec adaboosting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Random Forest avec AdaBoost\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def RandomForestAdaBoost(dictionnaire, Nb_estimRF, Nb_estimAB, Random_state):\n","    # Charger les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.2, random_state=Random_state)\n","\n","    # Créer un modèle Random Forest en faisant varier le n_estimators\n","    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n","    clf_rf.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Random Forest\n","    y_pred_rf = clf_rf.predict(X_test)\n","\n","    # Calculer la précision du modèle Random Forest\n","    \"\"\"print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\"\"\"\n","\n","    # Créer un modèle AdaBoost\n","    clf_ab = AdaBoostClassifier(n_estimators=Nb_estimAB, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n","\n","    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n","    clf_ab.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n","    y_pred_ab = clf_ab.predict(X_test)\n","\n","    # Calculer la précision du modèle AdaBoost\n","    \"\"\"print(\"Précision AdaBoost:\", accuracy_score(y_test, y_pred_ab))\"\"\"\n","   \n","    return accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_ab)"]},{"cell_type":"markdown","metadata":{},"source":["Graphe avec adaboosted random forest et sans "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Graphe RandomForest avec et sans AdaBoost\n","\n","import matplotlib.pyplot as plt\n","\n","N = np.arange(25,1200,25)\n","RF = []\n","AB = []\n","for i in range(len(N)):\n","    moyRF = []\n","    moyAB = []\n","    for j in range(10):\n","        RFAB = RandomForestAdaBoost(dict_label_bloc, 825, N[i], j) #825 meilleur paramètre pour RF\n","        moyRF.append(RFAB[0]) # RandomForest\n","        moyAB.append(RFAB[1]) # AdaBoost\n","    moyRF = np.mean(moyRF)\n","    moyAB = np.mean(moyAB)\n","    RF.append(moyRF)\n","    AB.append(moyAB)\n","\n","plt.plot(N, RF, color = 'green', marker = 'o', label = 'Random Forest')\n","plt.plot(N, AB, color = 'red', marker = 'o', label = 'AdaBoost')\n","plt.legend()\n","\n","print('Précision maximum RandomForest avec AdaBoost :', np.max(AB), ' atteint pour', N[np.argmax(AB)],' estimateurs que prend AdaBoost')"]},{"cell_type":"markdown","metadata":{},"source":["GradientBoosting and Xtreme gradient boosting of randomforest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Random Forest avec Extrem Grandient Boosting\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def RandomForestGradientBoosting(dictionnaire, Nb_estimRF, Nb_estimGB):\n","    # Charger les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n","\n","    # Créer un modèle Random Forest en faisant varier le n_estimators\n","    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n","    clf_rf.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Random Forest\n","    y_pred_rf = clf_rf.predict(X_test)\n","\n","    # Calculer la précision du modèle Random Forest\n","    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n","\n","    # Créer un modèle Gradient Boosting\n","    clf_gb = GradientBoostingClassifier(n_estimators=Nb_estimGB, learning_rate=0.1, max_depth=3)\n","\n","    # Entraîner le modèle sur l'ensemble d'apprentissage\n","    clf_gb.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Gradient Boosting\n","    y_pred_gb = clf_gb.predict(X_test)\n","\n","    # Calculer la précision du modèle Gradient Boosting\n","    print(\"Précision Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Random Forest avec Extrem Grandient Boosting\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def RandomForestGradientBoosting(dictionnaire, Nb_estimRF, Nb_estimGB):\n","    # Charger les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n","\n","    # Créer un modèle Random Forest en faisant varier le n_estimators\n","    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n","    clf_rf.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Random Forest\n","    y_pred_rf = clf_rf.predict(X_test)\n","\n","    # Calculer la précision du modèle Random Forest\n","    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n","\n","    # Créer un modèle Gradient Boosting\n","    clf_gb = GradientBoostingClassifier(n_estimators=Nb_estimGB, learning_rate=0.1, max_depth=3)\n","\n","    # Entraîner le modèle sur l'ensemble d'apprentissage\n","    clf_gb.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Gradient Boosting\n","    y_pred_gb = clf_gb.predict(X_test)\n","\n","    # Calculer la précision du modèle Gradient Boosting\n","    print(\"Précision Gradient Boosting:\", accuracy_score(y_test, y_pred_gb))\n"]},{"cell_type":"markdown","metadata":{},"source":["Extra trees and graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","def ExtraTrees(dictionnaire, Nb_estim, Random):\n","    # Chargez les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparez les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=Random)\n","\n","    # Créez un modèle Extra Trees Classifier en faisant varier le n__estimator\n","    clf = ExtraTreesClassifier(n_estimators=Nb_estim)\n","\n","    # Entraînez le modèle sur l'ensemble d'apprentissage\n","    clf.fit(X_train, y_train)\n","\n","    # Prédisez les sorties pour l'ensemble de test\n","    y_pred = clf.predict(X_test)\n","\n","    # Calculez la précision du modèle\n","    \"\"\"print(\"Précision :\", accuracy_score(y_test, y_pred))\"\"\"\n","    return accuracy_score(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Graphe Extra Trees en fonction du nombre d'estimateurs\n","\n","import matplotlib.pyplot as plt\n","\n","N = np.arange(50,1000,50)\n","ET = []\n","for i in range(len(N)):\n","    moy = []\n","    for j in range(10):\n","        moy.append(ExtraTrees(dict_label_bloc, N[i], j)) # random_state = j\n","    moy = np.mean(moy)\n","    ET.append(moy)\n","\n","plt.plot(N, ET, color = 'green', marker = 'o', label = 'Extra Trees')\n","plt.legend()\n","\n","print('Précision maximum :', np.max(ET), ' atteint pour', N[np.argmax(ET)],' estimateurs')\n","print('Variance de la précision RandomForest :', np.var(ET))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["randomforest(dict_label_bloc,650)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:35:37.319165Z","iopub.status.busy":"2023-12-08T18:35:37.318625Z","iopub.status.idle":"2023-12-08T18:35:37.332501Z","shell.execute_reply":"2023-12-08T18:35:37.331647Z","shell.execute_reply.started":"2023-12-08T18:35:37.319116Z"},"trusted":true},"outputs":[],"source":["# Random Forest avec AdaBoost et bagging\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def Rf_Ab_Bg(dictionnaire, Nb_estimRF, Nb_estimAB,Nb_estimBG):\n","    # Charger les données du dictionnaire\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n","\n","    ###RANDOM FOREST\n","    # Créer un modèle Random Forest en faisant varier le n_estimators\n","    clf_rf = RandomForestClassifier(n_estimators=Nb_estimRF)\n","    clf_rf.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec Random Forest\n","    y_pred_rf = clf_rf.predict(X_test)\n","\n","    # Calculer la précision du modèle Random Forest\n","    print(\"Précision Random Forest:\", accuracy_score(y_test, y_pred_rf))\n","    \n","    ###ADABOOST\n","    # Créer un modèle AdaBoost\n","    clf_ab = AdaBoostClassifier(n_estimators=Nb_estimAB, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n","\n","    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n","    clf_ab.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n","    y_pred_ab = clf_ab.predict(X_test)\n","\n","    # Calculer la précision du modèle AdaBoost\n","    print(\"Précision AdaBoost:\", accuracy_score(y_test, y_pred_ab))\n","    \n","    ###BAGGING\n","    ###ADABOOST\n","    # Créer un modèle Bagging\n","    clf_bg = BaggingClassifier(n_estimators=Nb_estimBG, base_estimator=RandomForestClassifier(n_estimators=Nb_estimRF))\n","\n","    # Entraîner le modèle AdaBoost sur l'ensemble d'apprentissage\n","    clf_bg.fit(X_train, y_train)\n","\n","    # Prédire les sorties pour l'ensemble de test avec AdaBoost\n","    y_pred_bg = clf_ab.predict(X_test)\n","\n","    # Calculer la précision du modèle AdaBoost\n","    print(\"Précision Bagging:\", accuracy_score(y_test, y_pred_ab))\n","    \n","    return(accuracy_score(y_test, y_pred_rf),accuracy_score(y_test, y_pred_ab),accuracy_score(y_test, y_pred_bg))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","Rf_Ab_Bg(dict_label_bloc, 650, 150,50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:35:45.170466Z","iopub.status.busy":"2023-12-08T18:35:45.170063Z"},"trusted":true},"outputs":[],"source":["\n","Y=[]\n","for i in range(10):\n","    Y.append(Rf_Ab_Bg(dict_label_bloc, 650, 150,40))\n","Z=np.array(Y)\n","print(np.shape(Z))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(Z)\n","\n","print(Z[:,0])\n","print(Z[:,1])\n","print(Z[:,2])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Assuming Z has three columns corresponding to Random Forest, AdaBoost, and Bagging\n","x = np.arange(0, 10, 1)\n","\n","# Création de la figure\n","plt.figure()\n","\n","# Tracé des courbes avec légendes\n","plt.plot(x, Z[:, 0], color='green', label='Random Forest')\n","plt.plot(x, Z[:, 1], color='orange', label='AdaBoost')\n","plt.plot(x, Z[:, 2], color='blue', label='Bagging')\n","\n","# Ajout de la légende\n","plt.legend()\n","\n","# Ajout des titres et étiquettes d'axe\n","plt.title(\"Comparison of AdaBoost, Bagging, and Random Forest over multiple classifications\")\n","plt.xlabel('Classification')\n","plt.ylabel('Accuracy score')\n","\n","# Affichage du graphique\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["Obersvation de la pertinence d'appliquer un Boosting ou un Bagging (ci dessus)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-08T18:11:51.340848Z","iopub.status.busy":"2023-12-08T18:11:51.340261Z","iopub.status.idle":"2023-12-08T18:11:51.34711Z","shell.execute_reply":"2023-12-08T18:11:51.346044Z","shell.execute_reply.started":"2023-12-08T18:11:51.340813Z"},"trusted":true},"outputs":[],"source":["def calculate_weights(X_test, y_test, classifiers):\n","    weights = {}\n","    for name, classifier in classifiers:\n","        y_pred_single = classifier.predict(X_test)\n","        accuracy_single = accuracy_score(y_test, y_pred_single)\n","        weights[name] = accuracy_single\n","    total_weight = sum(weights.values())\n","    return {name: weight / total_weight for name, weight in weights.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#VOTING WITH DIFFERENT CLASSIFICATIONS\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def voting(dictionnaire, classifiers):\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n","\n","    # Create a VotingClassifier with various types of classifiers\n","    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n","\n","    # Train each individual classifier before training the ensemble\n","    for name, classifier in classifiers:\n","        classifier.fit(X_train, y_train)\n","\n","    # Train the ensemble classifier\n","    voting_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = voting_classifier.predict(X_test)\n","\n","    # Evaluate accuracy for each classifier\n","    accuracies = {}\n","    for name, classifier in classifiers:\n","        y_pred_single = classifier.predict(X_test)\n","        accuracy_single = accuracy_score(y_test, y_pred_single)\n","        accuracies[name] = accuracy_single\n","        print(f'{name} Accuracy: {accuracy_single:.2f}')\n","\n","    # Evaluate overall ensemble accuracy\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f'Ensemble Accuracy: {accuracy:.2f}')\n","\n","    return accuracies, accuracy\n","\n","# Example usage\n","classifiers = [\n","    ('rf', RandomForestClassifier(n_estimators=440, random_state=1)),\n","    ('ada', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=440, random_state=1),n_estimators=10, random_state=2)),\n","    ('svc', SVC(probability=True, random_state=3)),\n","    ('dt', DecisionTreeClassifier(random_state=5)),\n","    ('lr', LogisticRegression(random_state=4)),\n","    ('knn', KNeighborsClassifier(n_neighbors=2)),\n","    ('nb', GaussianNB()),\n","    ('mlp', MLPClassifier(random_state=6)),\n","    ('bagging', BaggingClassifier(base_estimator=RandomForestClassifier(n_estimators=440, random_state=1), n_estimators=40, random_state=7)),\n","    ('extra_trees', ExtraTreesClassifier(n_estimators=100, random_state=9)),\n","]\n","\n","# Call the function with the classifiers\n","classifier_accuracies, ensemble_accuracy = voting(dict_label_bloc, classifiers)\n","\n","# Access individual classifier accuracies\n","print(\"\\nIndividual Classifier Accuracies:\")\n","for name, accuracy in classifier_accuracies.items():\n","    print(f'{name}: {accuracy:.2f}')\n","\n","# Access overall ensemble accuracy\n","print(f'\\nOverall Ensemble Accuracy: {ensemble_accuracy:.2f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["On eneleve les classifier donnant des scores inférieurs à 40%. Et on ajoute une notion de poids concernant les votes proportionnels à l'accuracy du classifieur"]},{"cell_type":"markdown","metadata":{},"source":["Let's try with only either Adaboost, rf or bagging because the classification is redundant"]},{"cell_type":"markdown","metadata":{},"source":["We have very similar results with way better speed"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#VOTING WITH DIFFERENT CLASSIFICATIONS\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","def voting(dictionnaire, classifiers):\n","    data_flat = [element for sous_liste in dictionnaire.values() for element in sous_liste]\n","    data_flat = np.array(data_flat)\n","    data_flat2 = data_flat.reshape(320, -1)\n","\n","    lab = np.array([[0] * 32, [1] * 32, [2] * 32, [3] * 32, [4] * 32, [5] * 32, [6] * 32, [7] * 32, [8] * 32, [9] * 32])\n","    lab = [element for sous_liste in lab for element in sous_liste]\n","\n","    for i in range(len(lab)):\n","        lab[i] = np.array(lab[i])\n","\n","    # Séparer les données en ensemble de test et d'apprentissage\n","    X_train, X_test, y_train, y_test = train_test_split(data_flat2, lab, test_size=0.25, random_state=7)\n","\n","    # Create a VotingClassifier with various types of classifiers\n","    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft')  # Use 'hard' for hard voting\n","\n","    # Train each individual classifier before training the ensemble\n","    for name, classifier in classifiers:\n","        classifier.fit(X_train, y_train)\n","\n","    # Train the ensemble classifier\n","    voting_classifier.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = voting_classifier.predict(X_test)\n","\n","    # Evaluate accuracy for each classifier\n","    accuracies = {}\n","    for name, classifier in classifiers:\n","        y_pred_single = classifier.predict(X_test)\n","        accuracy_single = accuracy_score(y_test, y_pred_single)\n","        accuracies[name] = accuracy_single\n","        print(f'{name} Accuracy: {accuracy_single:.2f}')\n","\n","    # Evaluate overall ensemble accuracy\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f'Ensemble Accuracy: {accuracy:.2f}')\n","    for name, classifier in classifiers:\n","        classifier.fit(X_train, y_train)\n","\n","    # Calculate weights based on accuracy\n","    classifier_weights = calculate_weights(X_test, y_test, classifiers)\n","    print(classifier_weights)\n","    return accuracies, accuracy\n","    voting_classifier = VotingClassifier(estimators=classifiers, voting='soft', weights=list(classifier_weights.values()))\n","\n","# Example usage\n","classifiers = [\n","    \n","    ('ABrf1', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=4), n_estimators=600, random_state=7)),\n","    ('ABrf2', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=6), n_estimators=600, random_state=8)),\n","    ('ABrf3', AdaBoostClassifier(base_estimator=RandomForestClassifier(n_estimators=825, random_state=4), n_estimators=600, random_state=9)),\n","    ]\n","\n","# Call the function with the classifiers\n","classifier_accuracies, ensemble_accuracy = voting(dict_label_bloc, classifiers)\n","\n","# Access individual classifier accuracies\n","print(\"\\nIndividual Classifier Accuracies:\")\n","for name, accuracy in classifier_accuracies.items():\n","    print(f'{name}: {accuracy:.2f}')\n","\n","# Access overall ensemble accuracy\n","print(f'\\nOverall Ensemble Accuracy: {ensemble_accuracy:.2f}')\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7009041,"sourceId":63982,"sourceType":"competition"}],"dockerImageVersionId":30587,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
